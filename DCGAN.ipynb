{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<ipython-input-4-ee2b9edf873d>, line 147)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-4-ee2b9edf873d>\"\u001b[0;36m, line \u001b[0;32m147\u001b[0m\n\u001b[0;31m    image_batch = X_train[index*BATCH_SIZE:(index+1)*BATCH_SIZE]\u001b[0m\n\u001b[0m                                                                ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.optimizers import SGD\n",
    "from keras.layers import Dense, Flatten, Convolution2D, MaxPooling2D, UpSampling2D\n",
    "from keras.layers.core import Activation\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import argparse\n",
    "import math\n",
    "\n",
    "\n",
    "theano.config.floatX = 'float32'\n",
    "DATA_LOC=r\"C:\\Users\\NAVPREET\\AnacondaProjects\\inpainting\\inpainting\"\n",
    "FTRAIN   = 'train2014'\n",
    "FVALID   = 'val2014'\n",
    "CAPTION  = 'dict_key_imgID_value_caps_train_and_valid.pkl'\n",
    "noise_dim=10\n",
    "\n",
    "\n",
    "def load(batch_idx = None, batch_size = None, valid = False):\n",
    "    '''Load data from FVALID if *valid* is True, otherwise load from FTRAIN.\n",
    "       Discard black and white examples by default.\n",
    "    '''\n",
    "    # Path to dataset\n",
    "    fname = FVALID if valid else FTRAIN\n",
    "    data_path = os.path.join(DATA_LOC, fname)\n",
    "    print data_path + \"/*.jpg\"\n",
    "    imgs = glob.glob(data_path + \"/*.jpg\")\n",
    "    if batch_idx != None:\n",
    "        batch_imgs = imgs[batch_idx*batch_size:(batch_idx+1)*batch_size]\n",
    "    else:\n",
    "        batch_imgs = imgs\n",
    "\n",
    "    # Initialize variables\n",
    "    X_int, y_int= [], []\n",
    "\n",
    "    for i, img_path in enumerate(batch_imgs):\n",
    "    # loading images from the given path\n",
    "        img = Image.open(img_path)\n",
    "        img_array = np.array(img)\n",
    "\n",
    "        # Get input/target from the images\n",
    "        center = (int(np.floor(img_array.shape[0] / 2.)), int(np.floor(img_array.shape[1] / 2.)))\n",
    "        if len(img_array.shape) == 3:\n",
    "            input = np.copy(img_array)\n",
    "            input[center[0]-16:center[0]+16, center[1]-16:center[1]+16, :] = 0\n",
    "            target = img_array[center[0]-16:center[0]+16, center[1] - 16:center[1]+16, :]\n",
    "        \n",
    "        X_int.append(input)\n",
    "        y_int.append(target)\n",
    "\n",
    "    X = np.asarray(X_int, dtype=np.float32) / 255.\n",
    "    y = np.asarray(y_int, dtype=np.float32) / 255.\n",
    "    X = np.transpose(X, (0,3,1,2))\n",
    "    y = np.transpose(y, (0,3,1,2))\n",
    "\n",
    "    return X, y\n",
    "\n",
    "\n",
    "\n",
    "def rebuilt_image(X, ypd, display = True): \n",
    "    \n",
    "    # Rearrange shapes, data type\n",
    "    X = np.asarray(255*X.transpose(0,2,3,1), np.uint8)\n",
    "    ypd = np.asarray(255*ypd.transpose(0,2,3,1), np.uint8)\n",
    "\n",
    "    collage = []\n",
    "\n",
    "    for i in range(len(X)):\n",
    "        img_array = X[i,:]\n",
    "        center = (int(np.floor(img_array.shape[0] / 2.)), int(np.floor(img_array.shape[1] / 2.)))\n",
    "        # Draw predicted image\n",
    "        imgpd = np.copy(img_array)\n",
    "        imgpd[center[0]-16:center[0]+16, center[1]-16:center[1]+16, :] = ypd[i,:]\n",
    "        if i == 0: collage = imgpd\n",
    "        else:      collage = np.hstack((collage, imgpd))\n",
    "\n",
    "    # Display images\n",
    "    if display:\n",
    "        Image.fromarray(collage).show()\n",
    "\n",
    "\n",
    "def genrator_model():\n",
    "    model=Sequential()\n",
    "    \n",
    "    model.add(Dense(input_shape=(Batch_size, noise_dim), units=3072, activation='relu'))\n",
    "    model.add(Reshape([32, 32, 3]))\n",
    "    model.add(BatchNormalization(Convolution2D(filters=32, kernel_size=(3, 3), padding='same', strides=(1,1), activation='relu')))\n",
    "    model.add(BatchNormalization(Convolution2D(filters=32, kernel_size=(5,5), padding='same', strides=(1,1), activation='relu')))\n",
    "    model.add(BatchNormalization(Convolution2D(filters=64, kernel_size=(3, 3), padding='same', strides=(1,1), activation='relu')))\n",
    "    model.add(BatchNormalization(Convolution2D(filters=64, kernel_size=(3,3), padding='same', strides=(1,1), activation='relu')))\n",
    "    model.add(BatchNormalization(Convolution2D(filters=32, kernel_size=(3,3), padding='same', strides=(1,1), activation='relu')))\n",
    "    model.add(BatchNormalization(Convolution2D(filters=32, kernel_size=(3,3), padding='same', strides=(1,1), activation='relu')))\n",
    "    model.add(BatchNormalization(Convolution2D(filters=64, kernel_size=(3,3), padding='same', strides=(1,1), activation='relu')))\n",
    "    model.add(BatchNormalization(Convolution2D(filters=3, kernel_size=(3,3), padding='same', strides=(1,1), activation='relu')))\n",
    "    model.add(Reshape([32,32, 3]))\n",
    "              \n",
    "    \n",
    "def discriminator_model():\n",
    "    model=Sequential()\n",
    "    model.add(BatchNormalization(Convolution2D(filters=32, input_shape=(32, 32, 3), kernel_size=(3,3), activation='relu', strides=(1,1), padding='same')))\n",
    "    model.add(BatchNormalization(Convolution2D(filters=32, kernel_size=(3,3), activation='relu', strides=(1,1), padding='same')))\n",
    "    model.add(BatchNormalization(Convolution2D(filters=64, kernel_size=(3,3), activation='relu', strides=(2,2), padding='same')))\n",
    "    model.add(BatchNormalization(Convolution2D(filters=64, kernel_size=(3,3), activation='relu', strides=(1,1), padding='same')))\n",
    "    model.add(BatchNormalization(Convolution2D(filters=32, kernel_size=(3,3), activation='relu', strides=(1,1), padding='same')))\n",
    "    model.add(BatchNormalization(Convolution2D(filters=32, kernel_size=(3,3), activation='relu', strides=(2,2), padding='same')))\n",
    "    model.add(BatchNormalization(Convolution2D(filters=32, kernel_size=(3,3), activation='relu', strides=(1,1), padding='same')))\n",
    "    model.add(BatchNormalization(Convolution2D(filters=32, kernel_size=(3,3), activation='relu', strides=(1,1), padding='same')))\n",
    "    model.add(BatchNormalization(Convolution2D(filters=32, kernel_size=(3,3), activation='relu', strides=(2,2), padding='same')))\n",
    "    model.add(BatchNormalization(Convolution2D(filters=32, kernel_size=(3,3), activation='relu', strides=(1,1), padding='same')))\n",
    "    model.add(BatchNormalization(Convolution2D(filters=32, kernel_size=(3,3), activation='relu', strides=(1,1), padding='same')))\n",
    "    model.add(BatchNormalization(Convolution2D(filters=32, kernel_size=(3,3), activation='relu', strides=(1,1), padding='same')))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    return model\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "def generator_containing_discriminator():\n",
    "    model = Sequential()\n",
    "    model.add(generator)\n",
    "    discriminator.trainable = False\n",
    "    model.add(discriminator)\n",
    "    return model\n",
    "              \n",
    "              \n",
    "def Train(Batch_size):\n",
    "    X_train, y_train=load(1,10)\n",
    "    X_val, y_val=load(1, 10, valid=True)\n",
    "    discriminator = discriminator_model()\n",
    "    generator = generator_model()\n",
    "    discriminator_on_generator = \\\n",
    "        generator_containing_discriminator(generator, discriminator)\n",
    "    d_optim = SGD(lr=0.002,momentum=0.9, nesterov=True)\n",
    "    g_optim = SGD(lr=0.002,momentum=0.9, nesterov=True)\n",
    "    generator.compile(loss='mean_squared_error', optimizer='SGD')\n",
    "    discriminator_on_generator.compile(\n",
    "        loss='mean_squared_error', optimizer=g_optim)\n",
    "    discriminator.trainable = True\n",
    "    discriminator.compile(loss='mean_squared_error', optimizer=d_optim)\n",
    "    img=load(1,1, valid=True)\n",
    "    generated_image=generator.predict(img)\n",
    "    noise = np.zeros((BATCH_SIZE, 10))\n",
    "    for epoch in range(10):\n",
    "              for index in range(int(X_train.shape[0]/BATCH_SIZE)):\n",
    "                  for i in range(BATCH_SIZE):\n",
    "                    noise[i, :] = np.random.uniform(-1, 1, 10)\n",
    "                image_batch = X_train[index*BATCH_SIZE:(index+1)*BATCH_SIZE]\n",
    "                generated_images = generator.predict(noise, verbose=0)\n",
    "            if index % 20 == 0:\n",
    "                image = combine_images(generated_images)\n",
    "                image = image*127.5+127.5\n",
    "                Image.fromarray(image.astype(np.uint8)).save(\n",
    "                    str(epoch)+\"_\"+str(index)+\".png\")\n",
    "            X = np.concatenate((image_batch, generated_images))\n",
    "            y = [1] * BATCH_SIZE + [0] * BATCH_SIZE\n",
    "            d_loss = discriminator.train_on_batch(X, y)\n",
    "            for i in range(BATCH_SIZE):\n",
    "                noise[i, :] = np.random.uniform(-1, 1, 10)\n",
    "            discriminator.trainable = False\n",
    "            g_loss = discriminator_on_generator.train_on_batch(\n",
    "                noise, [1] * BATCH_SIZE)\n",
    "            discriminator.trainable = True\n",
    "            print(\"batch %d g_loss : %f\" % (index, g_loss))\n",
    "            if index % 10 == 9:\n",
    "                generator.save_weights('generator', True)\n",
    "                discriminator.save_weights('discriminator', True)  \n",
    "              \n",
    "\n",
    "    def generate(BATCH_SIZE, nice=False):\n",
    "    generator = generator_model()\n",
    "    generator.compile(loss='mean_squared_error', optimizer=\"SGD\")\n",
    "    generator.load_weights('generator')\n",
    "    if nice:\n",
    "        discriminator = discriminator_model()\n",
    "        discriminator.compile(loss='mean_squared_error', optimizer=\"SGD\") \n",
    "        discriminator.load_weights('discriminator')\n",
    "        noise = np.zeros((BATCH_SIZE*20, 10))\n",
    "        for i in range(BATCH_SIZE*20):\n",
    "            noise[i, :] = np.random.uniform(-1, 1, 10)\n",
    "        generated_images = generator.predict(noise, verbose=1)\n",
    "        d_pret = discriminator.predict(generated_images, verbose=1)\n",
    "        index = np.arange(0, BATCH_SIZE*20)\n",
    "        index.resize((BATCH_SIZE*20, 1))\n",
    "        pre_with_index = list(np.append(d_pret, index, axis=1))\n",
    "        pre_with_index.sort(key=lambda x: x[0], reverse=True)\n",
    "        nice_images = np.zeros((BATCH_SIZE, 1) +\n",
    "                               (generated_images.shape[2:]), dtype=np.float32)\n",
    "        for i in range(int(BATCH_SIZE)):\n",
    "            idx = int(pre_with_index[i][1])\n",
    "            nice_images[i, 0, :, :] = generated_images[idx, 0, :, :]\n",
    "        image = combine_images(nice_images)\n",
    "    else:\n",
    "        noise = np.zeros((BATCH_SIZE, 10))\n",
    "        for i in range(BATCH_SIZE):\n",
    "            noise[i, :] = np.random.uniform(-1, 1, 10)\n",
    "        generated_images = generator.predict(noise, verbose=1)\n",
    "        image = combine_images(generated_images)\n",
    "    image = image*255+255\n",
    "    Image.fromarray(image.astype(np.uint8)).save(\n",
    "        \"gen_image.png\")\n",
    "\n",
    "              \n",
    "def get_args():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--mode\", type=str)\n",
    "    parser.add_argument(\"--batch_size\", type=int, default=64)\n",
    "    parser.add_argument(\"--nice\", dest=\"nice\", action=\"store_true\")\n",
    "    parser.set_defaults(nice=False)\n",
    "    args = parser.parse_args()\n",
    "    return args\n",
    "              \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    args = get_args()\n",
    "    if args.mode == \"train\":\n",
    "        train(BATCH_SIZE=args.batch_size)\n",
    "    elif args.mode == \"generate\":\n",
    "        generate(BATCH_SIZE=args.batch_size, nice=args.nice)\n",
    "              \n",
    "              \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
